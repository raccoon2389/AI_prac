{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext watermark\n",
    "# %reload_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %watermark -v -p dateutil,numpy,matplotlib,pandas,torch,tqdm,TaPR_pkg,cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "\n",
    "import dateutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import trange\n",
    "from TaPR_pkg import etapr\n",
    "from sklearn.preprocessing import Normalizer,MinMaxScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[WindowsPath('training/train1.csv'), WindowsPath('training/train2.csv'), WindowsPath('training/train3.csv')]\n[WindowsPath('testing/test1.csv'), WindowsPath('testing/test2.csv'), WindowsPath('testing/test3.csv'), WindowsPath('testing/test4.csv')]\n[WindowsPath('validation/validation.csv')]\n"
    }
   ],
   "source": [
    "TRAIN_DATASET = sorted([x for x in Path(\"training/\").glob(\"*.csv\")])\n",
    "print(TRAIN_DATASET)\n",
    "\n",
    "TEST_DATASET = sorted([x for x in Path(\"testing/\").glob(\"*.csv\")])\n",
    "print(TEST_DATASET)\n",
    "\n",
    "VALIDATION_DATASET = sorted([x for x in Path(\"validation/\").glob(\"*.csv\")])\n",
    "print(VALIDATION_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_from_csv(target):\n",
    "    return pd.read_csv(target).rename(columns=lambda x: x.strip())\n",
    "\n",
    "def dataframe_from_csvs(targets):\n",
    "    return pd.concat([dataframe_from_csv(x) for x in targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                       time        C01  C02  C03       C04     C05      C06  \\\n0       2020-07-11 00:00:00  395.19528   12   10  52.80456 -1.2648 -1.87531   \n1       2020-07-11 00:00:01  395.14420   12   10  52.78931 -1.3147 -1.88294   \n2       2020-07-11 00:00:02  395.14420   12   10  52.79694 -1.4032 -1.88294   \n3       2020-07-11 00:00:03  395.19528   12   10  52.79694 -1.6074 -1.88294   \n4       2020-07-11 00:00:04  395.34866   12   10  52.79694 -1.7811 -1.88294   \n...                     ...        ...  ...  ...       ...     ...      ...   \n478796  2020-08-10 10:59:56  387.27219   12   10  66.72057 -0.9331 -1.84479   \n478797  2020-08-10 10:59:57  387.52774   12   10  66.72057 -0.9996 -1.84479   \n478798  2020-08-10 10:59:58  387.47665   12   10  66.72057 -1.2560 -1.84479   \n478799  2020-08-10 10:59:59  387.73221   12   10  66.72057 -1.4912 -1.84479   \n478800  2020-08-10 11:00:00  387.52774   12   10  66.72057 -1.5727 -1.84479   \n\n              C07       C08      C09  ...        C70  C71      C72       C73  \\\n0       779.59595  28.02645  10832.0  ...  808.29620  0.0  1.36810   8.79882   \n1       780.67328  28.02473  10984.0  ...  819.16809  0.0  1.36810   8.78811   \n2       780.06574  28.02817  11120.0  ...  823.51697  0.0  1.36734   8.81787   \n3       780.15265  28.02301  11256.0  ...  823.95172  0.0  1.36734   8.87493   \n4       781.83160  28.03595  11384.0  ...  827.86560  0.0  1.36810   8.83838   \n...           ...       ...      ...  ...        ...  ...      ...       ...   \n478796  781.87915  28.02389    880.0  ...  944.84705  0.0  1.32843  15.17817   \n478797  787.65070  28.02385    840.0  ...  940.49835  0.0  1.32843  15.17344   \n478798  788.50256  28.03085    792.0  ...  935.71472  0.0  1.32919  15.16443   \n478799  785.80316  28.02649    752.0  ...  944.84705  0.0  1.32843  15.09001   \n478800  780.21381  28.02476    720.0  ...  951.80505  0.0  1.32919  15.08672   \n\n             C74       C75        C76        C77      C78     C79  \n0       35.43700  12.01782  305.03113  301.35992  33.6555  6.0951  \n1       35.45227  12.01782  304.27161  297.43567  33.6555  5.9262  \n2       35.45227  12.01782  303.89179  298.66534  33.6555  5.8101  \n3       35.43700  12.01782  303.67474  298.06860  33.6555  5.7509  \n4       35.45227  12.01782  303.22266  296.53137  33.6555  5.8547  \n...          ...       ...        ...        ...      ...     ...  \n478796  35.14710  11.79657  316.89453  296.54950  32.0000  6.6026  \n478797  35.13183  11.79657  315.59247  296.15161  32.0000  6.3894  \n478798  35.13183  11.79657  313.92865  293.40277  32.0000  6.2584  \n478799  35.14710  11.79657  315.61054  302.58972  32.0000  6.4150  \n478800  35.14710  11.79657  317.23816  309.00964  32.0000  6.6288  \n\n[921603 rows x 80 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>C01</th>\n      <th>C02</th>\n      <th>C03</th>\n      <th>C04</th>\n      <th>C05</th>\n      <th>C06</th>\n      <th>C07</th>\n      <th>C08</th>\n      <th>C09</th>\n      <th>...</th>\n      <th>C70</th>\n      <th>C71</th>\n      <th>C72</th>\n      <th>C73</th>\n      <th>C74</th>\n      <th>C75</th>\n      <th>C76</th>\n      <th>C77</th>\n      <th>C78</th>\n      <th>C79</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2020-07-11 00:00:00</td>\n      <td>395.19528</td>\n      <td>12</td>\n      <td>10</td>\n      <td>52.80456</td>\n      <td>-1.2648</td>\n      <td>-1.87531</td>\n      <td>779.59595</td>\n      <td>28.02645</td>\n      <td>10832.0</td>\n      <td>...</td>\n      <td>808.29620</td>\n      <td>0.0</td>\n      <td>1.36810</td>\n      <td>8.79882</td>\n      <td>35.43700</td>\n      <td>12.01782</td>\n      <td>305.03113</td>\n      <td>301.35992</td>\n      <td>33.6555</td>\n      <td>6.0951</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2020-07-11 00:00:01</td>\n      <td>395.14420</td>\n      <td>12</td>\n      <td>10</td>\n      <td>52.78931</td>\n      <td>-1.3147</td>\n      <td>-1.88294</td>\n      <td>780.67328</td>\n      <td>28.02473</td>\n      <td>10984.0</td>\n      <td>...</td>\n      <td>819.16809</td>\n      <td>0.0</td>\n      <td>1.36810</td>\n      <td>8.78811</td>\n      <td>35.45227</td>\n      <td>12.01782</td>\n      <td>304.27161</td>\n      <td>297.43567</td>\n      <td>33.6555</td>\n      <td>5.9262</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020-07-11 00:00:02</td>\n      <td>395.14420</td>\n      <td>12</td>\n      <td>10</td>\n      <td>52.79694</td>\n      <td>-1.4032</td>\n      <td>-1.88294</td>\n      <td>780.06574</td>\n      <td>28.02817</td>\n      <td>11120.0</td>\n      <td>...</td>\n      <td>823.51697</td>\n      <td>0.0</td>\n      <td>1.36734</td>\n      <td>8.81787</td>\n      <td>35.45227</td>\n      <td>12.01782</td>\n      <td>303.89179</td>\n      <td>298.66534</td>\n      <td>33.6555</td>\n      <td>5.8101</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2020-07-11 00:00:03</td>\n      <td>395.19528</td>\n      <td>12</td>\n      <td>10</td>\n      <td>52.79694</td>\n      <td>-1.6074</td>\n      <td>-1.88294</td>\n      <td>780.15265</td>\n      <td>28.02301</td>\n      <td>11256.0</td>\n      <td>...</td>\n      <td>823.95172</td>\n      <td>0.0</td>\n      <td>1.36734</td>\n      <td>8.87493</td>\n      <td>35.43700</td>\n      <td>12.01782</td>\n      <td>303.67474</td>\n      <td>298.06860</td>\n      <td>33.6555</td>\n      <td>5.7509</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2020-07-11 00:00:04</td>\n      <td>395.34866</td>\n      <td>12</td>\n      <td>10</td>\n      <td>52.79694</td>\n      <td>-1.7811</td>\n      <td>-1.88294</td>\n      <td>781.83160</td>\n      <td>28.03595</td>\n      <td>11384.0</td>\n      <td>...</td>\n      <td>827.86560</td>\n      <td>0.0</td>\n      <td>1.36810</td>\n      <td>8.83838</td>\n      <td>35.45227</td>\n      <td>12.01782</td>\n      <td>303.22266</td>\n      <td>296.53137</td>\n      <td>33.6555</td>\n      <td>5.8547</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>478796</th>\n      <td>2020-08-10 10:59:56</td>\n      <td>387.27219</td>\n      <td>12</td>\n      <td>10</td>\n      <td>66.72057</td>\n      <td>-0.9331</td>\n      <td>-1.84479</td>\n      <td>781.87915</td>\n      <td>28.02389</td>\n      <td>880.0</td>\n      <td>...</td>\n      <td>944.84705</td>\n      <td>0.0</td>\n      <td>1.32843</td>\n      <td>15.17817</td>\n      <td>35.14710</td>\n      <td>11.79657</td>\n      <td>316.89453</td>\n      <td>296.54950</td>\n      <td>32.0000</td>\n      <td>6.6026</td>\n    </tr>\n    <tr>\n      <th>478797</th>\n      <td>2020-08-10 10:59:57</td>\n      <td>387.52774</td>\n      <td>12</td>\n      <td>10</td>\n      <td>66.72057</td>\n      <td>-0.9996</td>\n      <td>-1.84479</td>\n      <td>787.65070</td>\n      <td>28.02385</td>\n      <td>840.0</td>\n      <td>...</td>\n      <td>940.49835</td>\n      <td>0.0</td>\n      <td>1.32843</td>\n      <td>15.17344</td>\n      <td>35.13183</td>\n      <td>11.79657</td>\n      <td>315.59247</td>\n      <td>296.15161</td>\n      <td>32.0000</td>\n      <td>6.3894</td>\n    </tr>\n    <tr>\n      <th>478798</th>\n      <td>2020-08-10 10:59:58</td>\n      <td>387.47665</td>\n      <td>12</td>\n      <td>10</td>\n      <td>66.72057</td>\n      <td>-1.2560</td>\n      <td>-1.84479</td>\n      <td>788.50256</td>\n      <td>28.03085</td>\n      <td>792.0</td>\n      <td>...</td>\n      <td>935.71472</td>\n      <td>0.0</td>\n      <td>1.32919</td>\n      <td>15.16443</td>\n      <td>35.13183</td>\n      <td>11.79657</td>\n      <td>313.92865</td>\n      <td>293.40277</td>\n      <td>32.0000</td>\n      <td>6.2584</td>\n    </tr>\n    <tr>\n      <th>478799</th>\n      <td>2020-08-10 10:59:59</td>\n      <td>387.73221</td>\n      <td>12</td>\n      <td>10</td>\n      <td>66.72057</td>\n      <td>-1.4912</td>\n      <td>-1.84479</td>\n      <td>785.80316</td>\n      <td>28.02649</td>\n      <td>752.0</td>\n      <td>...</td>\n      <td>944.84705</td>\n      <td>0.0</td>\n      <td>1.32843</td>\n      <td>15.09001</td>\n      <td>35.14710</td>\n      <td>11.79657</td>\n      <td>315.61054</td>\n      <td>302.58972</td>\n      <td>32.0000</td>\n      <td>6.4150</td>\n    </tr>\n    <tr>\n      <th>478800</th>\n      <td>2020-08-10 11:00:00</td>\n      <td>387.52774</td>\n      <td>12</td>\n      <td>10</td>\n      <td>66.72057</td>\n      <td>-1.5727</td>\n      <td>-1.84479</td>\n      <td>780.21381</td>\n      <td>28.02476</td>\n      <td>720.0</td>\n      <td>...</td>\n      <td>951.80505</td>\n      <td>0.0</td>\n      <td>1.32919</td>\n      <td>15.08672</td>\n      <td>35.14710</td>\n      <td>11.79657</td>\n      <td>317.23816</td>\n      <td>309.00964</td>\n      <td>32.0000</td>\n      <td>6.6288</td>\n    </tr>\n  </tbody>\n</table>\n<p>921603 rows × 80 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "TRAIN_DF_RAW = dataframe_from_csvs(TRAIN_DATASET)\n",
    "TRAIN_DF_RAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Index(['C01', 'C02', 'C03', 'C04', 'C05', 'C06', 'C07', 'C08', 'C09', 'C10',\n       'C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20',\n       'C21', 'C22', 'C23', 'C24', 'C25', 'C26', 'C27', 'C28', 'C29', 'C30',\n       'C31', 'C32', 'C33', 'C34', 'C35', 'C36', 'C37', 'C38', 'C39', 'C40',\n       'C41', 'C42', 'C43', 'C44', 'C45', 'C46', 'C47', 'C48', 'C49', 'C50',\n       'C51', 'C52', 'C53', 'C54', 'C55', 'C56', 'C57', 'C58', 'C59', 'C60',\n       'C61', 'C62', 'C63', 'C64', 'C65', 'C66', 'C67', 'C68', 'C69', 'C70',\n       'C71', 'C72', 'C73', 'C74', 'C75', 'C76', 'C77', 'C78', 'C79'],\n      dtype='object')"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "TIMESTAMP_FIELD = \"time\"\n",
    "IDSTAMP_FIELD = 'id'\n",
    "ATTACK_FIELD = \"attack\"\n",
    "VALID_COLUMNS_IN_TRAIN_DATASET = TRAIN_DF_RAW.columns.drop([TIMESTAMP_FIELD])\n",
    "VALID_COLUMNS_IN_TRAIN_DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_MIN = TRAIN_DF_RAW[VALID_COLUMNS_IN_TRAIN_DATASET].min()\n",
    "TAG_MAX = TRAIN_DF_RAW[VALID_COLUMNS_IN_TRAIN_DATASET].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    ndf = df.copy()\n",
    "    for c in df.columns:\n",
    "        if TAG_MIN[c] == TAG_MAX[c]:\n",
    "            ndf[c] = df[c] - TAG_MIN[c]\n",
    "        else:\n",
    "            ndf[c] = (df[c] - TAG_MIN[c]) / (TAG_MAX[c] - TAG_MIN[c])\n",
    "    return ndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize(df):\n",
    "#     ndf = df.values\n",
    "#     scaler = MinMaxScaler()\n",
    "#     ndf = scaler.fit_transform(ndf)\n",
    "#     print(ndf.shape)\n",
    "#     ndf = pd.DataFrame(ndf,columns=VALID_COLUMNS_IN_TRAIN_DATASET,index=None)\n",
    "#     return ndf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "              C01  C02  C03       C04     C05      C06        C07       C08  \\\n0       395.19528   12   10  52.80456 -1.2648 -1.87531  779.59595  28.02645   \n1       395.14420   12   10  52.78931 -1.3147 -1.88294  780.67328  28.02473   \n2       395.14420   12   10  52.79694 -1.4032 -1.88294  780.06574  28.02817   \n3       395.19528   12   10  52.79694 -1.6074 -1.88294  780.15265  28.02301   \n4       395.34866   12   10  52.79694 -1.7811 -1.88294  781.83160  28.03595   \n...           ...  ...  ...       ...     ...      ...        ...       ...   \n478796  387.27219   12   10  66.72057 -0.9331 -1.84479  781.87915  28.02389   \n478797  387.52774   12   10  66.72057 -0.9996 -1.84479  787.65070  28.02385   \n478798  387.47665   12   10  66.72057 -1.2560 -1.84479  788.50256  28.03085   \n478799  387.73221   12   10  66.72057 -1.4912 -1.84479  785.80316  28.02649   \n478800  387.52774   12   10  66.72057 -1.5727 -1.84479  780.21381  28.02476   \n\n            C09     C10  ...        C70  C71      C72       C73       C74  \\\n0       10832.0 -3.0660  ...  808.29620  0.0  1.36810   8.79882  35.43700   \n1       10984.0 -2.9721  ...  819.16809  0.0  1.36810   8.78811  35.45227   \n2       11120.0 -2.9857  ...  823.51697  0.0  1.36734   8.81787  35.45227   \n3       11256.0 -3.2166  ...  823.95172  0.0  1.36734   8.87493  35.43700   \n4       11384.0 -3.5613  ...  827.86560  0.0  1.36810   8.83838  35.45227   \n...         ...     ...  ...        ...  ...      ...       ...       ...   \n478796    880.0 -2.6936  ...  944.84705  0.0  1.32843  15.17817  35.14710   \n478797    840.0 -2.5863  ...  940.49835  0.0  1.32843  15.17344  35.13183   \n478798    792.0 -2.8287  ...  935.71472  0.0  1.32919  15.16443  35.13183   \n478799    752.0 -3.3212  ...  944.84705  0.0  1.32843  15.09001  35.14710   \n478800    720.0 -3.6418  ...  951.80505  0.0  1.32919  15.08672  35.14710   \n\n             C75        C76        C77      C78     C79  \n0       12.01782  305.03113  301.35992  33.6555  6.0951  \n1       12.01782  304.27161  297.43567  33.6555  5.9262  \n2       12.01782  303.89179  298.66534  33.6555  5.8101  \n3       12.01782  303.67474  298.06860  33.6555  5.7509  \n4       12.01782  303.22266  296.53137  33.6555  5.8547  \n...          ...        ...        ...      ...     ...  \n478796  11.79657  316.89453  296.54950  32.0000  6.6026  \n478797  11.79657  315.59247  296.15161  32.0000  6.3894  \n478798  11.79657  313.92865  293.40277  32.0000  6.2584  \n478799  11.79657  315.61054  302.58972  32.0000  6.4150  \n478800  11.79657  317.23816  309.00964  32.0000  6.6288  \n\n[921603 rows x 79 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>C01</th>\n      <th>C02</th>\n      <th>C03</th>\n      <th>C04</th>\n      <th>C05</th>\n      <th>C06</th>\n      <th>C07</th>\n      <th>C08</th>\n      <th>C09</th>\n      <th>C10</th>\n      <th>...</th>\n      <th>C70</th>\n      <th>C71</th>\n      <th>C72</th>\n      <th>C73</th>\n      <th>C74</th>\n      <th>C75</th>\n      <th>C76</th>\n      <th>C77</th>\n      <th>C78</th>\n      <th>C79</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>395.19528</td>\n      <td>12</td>\n      <td>10</td>\n      <td>52.80456</td>\n      <td>-1.2648</td>\n      <td>-1.87531</td>\n      <td>779.59595</td>\n      <td>28.02645</td>\n      <td>10832.0</td>\n      <td>-3.0660</td>\n      <td>...</td>\n      <td>808.29620</td>\n      <td>0.0</td>\n      <td>1.36810</td>\n      <td>8.79882</td>\n      <td>35.43700</td>\n      <td>12.01782</td>\n      <td>305.03113</td>\n      <td>301.35992</td>\n      <td>33.6555</td>\n      <td>6.0951</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>395.14420</td>\n      <td>12</td>\n      <td>10</td>\n      <td>52.78931</td>\n      <td>-1.3147</td>\n      <td>-1.88294</td>\n      <td>780.67328</td>\n      <td>28.02473</td>\n      <td>10984.0</td>\n      <td>-2.9721</td>\n      <td>...</td>\n      <td>819.16809</td>\n      <td>0.0</td>\n      <td>1.36810</td>\n      <td>8.78811</td>\n      <td>35.45227</td>\n      <td>12.01782</td>\n      <td>304.27161</td>\n      <td>297.43567</td>\n      <td>33.6555</td>\n      <td>5.9262</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>395.14420</td>\n      <td>12</td>\n      <td>10</td>\n      <td>52.79694</td>\n      <td>-1.4032</td>\n      <td>-1.88294</td>\n      <td>780.06574</td>\n      <td>28.02817</td>\n      <td>11120.0</td>\n      <td>-2.9857</td>\n      <td>...</td>\n      <td>823.51697</td>\n      <td>0.0</td>\n      <td>1.36734</td>\n      <td>8.81787</td>\n      <td>35.45227</td>\n      <td>12.01782</td>\n      <td>303.89179</td>\n      <td>298.66534</td>\n      <td>33.6555</td>\n      <td>5.8101</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>395.19528</td>\n      <td>12</td>\n      <td>10</td>\n      <td>52.79694</td>\n      <td>-1.6074</td>\n      <td>-1.88294</td>\n      <td>780.15265</td>\n      <td>28.02301</td>\n      <td>11256.0</td>\n      <td>-3.2166</td>\n      <td>...</td>\n      <td>823.95172</td>\n      <td>0.0</td>\n      <td>1.36734</td>\n      <td>8.87493</td>\n      <td>35.43700</td>\n      <td>12.01782</td>\n      <td>303.67474</td>\n      <td>298.06860</td>\n      <td>33.6555</td>\n      <td>5.7509</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>395.34866</td>\n      <td>12</td>\n      <td>10</td>\n      <td>52.79694</td>\n      <td>-1.7811</td>\n      <td>-1.88294</td>\n      <td>781.83160</td>\n      <td>28.03595</td>\n      <td>11384.0</td>\n      <td>-3.5613</td>\n      <td>...</td>\n      <td>827.86560</td>\n      <td>0.0</td>\n      <td>1.36810</td>\n      <td>8.83838</td>\n      <td>35.45227</td>\n      <td>12.01782</td>\n      <td>303.22266</td>\n      <td>296.53137</td>\n      <td>33.6555</td>\n      <td>5.8547</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>478796</th>\n      <td>387.27219</td>\n      <td>12</td>\n      <td>10</td>\n      <td>66.72057</td>\n      <td>-0.9331</td>\n      <td>-1.84479</td>\n      <td>781.87915</td>\n      <td>28.02389</td>\n      <td>880.0</td>\n      <td>-2.6936</td>\n      <td>...</td>\n      <td>944.84705</td>\n      <td>0.0</td>\n      <td>1.32843</td>\n      <td>15.17817</td>\n      <td>35.14710</td>\n      <td>11.79657</td>\n      <td>316.89453</td>\n      <td>296.54950</td>\n      <td>32.0000</td>\n      <td>6.6026</td>\n    </tr>\n    <tr>\n      <th>478797</th>\n      <td>387.52774</td>\n      <td>12</td>\n      <td>10</td>\n      <td>66.72057</td>\n      <td>-0.9996</td>\n      <td>-1.84479</td>\n      <td>787.65070</td>\n      <td>28.02385</td>\n      <td>840.0</td>\n      <td>-2.5863</td>\n      <td>...</td>\n      <td>940.49835</td>\n      <td>0.0</td>\n      <td>1.32843</td>\n      <td>15.17344</td>\n      <td>35.13183</td>\n      <td>11.79657</td>\n      <td>315.59247</td>\n      <td>296.15161</td>\n      <td>32.0000</td>\n      <td>6.3894</td>\n    </tr>\n    <tr>\n      <th>478798</th>\n      <td>387.47665</td>\n      <td>12</td>\n      <td>10</td>\n      <td>66.72057</td>\n      <td>-1.2560</td>\n      <td>-1.84479</td>\n      <td>788.50256</td>\n      <td>28.03085</td>\n      <td>792.0</td>\n      <td>-2.8287</td>\n      <td>...</td>\n      <td>935.71472</td>\n      <td>0.0</td>\n      <td>1.32919</td>\n      <td>15.16443</td>\n      <td>35.13183</td>\n      <td>11.79657</td>\n      <td>313.92865</td>\n      <td>293.40277</td>\n      <td>32.0000</td>\n      <td>6.2584</td>\n    </tr>\n    <tr>\n      <th>478799</th>\n      <td>387.73221</td>\n      <td>12</td>\n      <td>10</td>\n      <td>66.72057</td>\n      <td>-1.4912</td>\n      <td>-1.84479</td>\n      <td>785.80316</td>\n      <td>28.02649</td>\n      <td>752.0</td>\n      <td>-3.3212</td>\n      <td>...</td>\n      <td>944.84705</td>\n      <td>0.0</td>\n      <td>1.32843</td>\n      <td>15.09001</td>\n      <td>35.14710</td>\n      <td>11.79657</td>\n      <td>315.61054</td>\n      <td>302.58972</td>\n      <td>32.0000</td>\n      <td>6.4150</td>\n    </tr>\n    <tr>\n      <th>478800</th>\n      <td>387.52774</td>\n      <td>12</td>\n      <td>10</td>\n      <td>66.72057</td>\n      <td>-1.5727</td>\n      <td>-1.84479</td>\n      <td>780.21381</td>\n      <td>28.02476</td>\n      <td>720.0</td>\n      <td>-3.6418</td>\n      <td>...</td>\n      <td>951.80505</td>\n      <td>0.0</td>\n      <td>1.32919</td>\n      <td>15.08672</td>\n      <td>35.14710</td>\n      <td>11.79657</td>\n      <td>317.23816</td>\n      <td>309.00964</td>\n      <td>32.0000</td>\n      <td>6.6288</td>\n    </tr>\n  </tbody>\n</table>\n<p>921603 rows × 79 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "TRAIN_DF_RAW[VALID_COLUMNS_IN_TRAIN_DATASET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "             C01  C02  C03       C04       C05       C06       C07       C08  \\\n0       0.378953  0.0  0.0  0.227071  0.372380  0.000230  0.386721  0.410567   \n1       0.378504  0.0  0.0  0.226596  0.353516  0.000161  0.399074  0.364415   \n2       0.378463  0.0  0.0  0.226789  0.318663  0.000154  0.393283  0.451729   \n3       0.378904  0.0  0.0  0.226808  0.238782  0.000154  0.393697  0.323289   \n4       0.380282  0.0  0.0  0.226810  0.165794  0.000154  0.412796  0.654203   \n...          ...  ...  ...       ...       ...       ...       ...       ...   \n478796  0.302372  0.0  0.0  0.703684  0.509016  0.000538  0.420902  0.355242   \n478797  0.304595  0.0  0.0  0.703684  0.485295  0.000538  0.481569  0.335967   \n478798  0.304373  0.0  0.0  0.703684  0.386965  0.000538  0.497306  0.519990   \n478799  0.306574  0.0  0.0  0.703684  0.289108  0.000538  0.468238  0.422572   \n478800  0.305015  0.0  0.0  0.703684  0.248821  0.000538  0.401886  0.366873   \n\n             C09       C10  ...       C70  C71       C72       C73       C74  \\\n0       0.784144  0.508049  ...  0.584892  0.0  0.326835  0.254687  0.331076   \n1       0.794139  0.540538  ...  0.592044  0.0  0.326835  0.254315  0.337223   \n2       0.803903  0.538802  ...  0.595523  0.0  0.326387  0.255304  0.337777   \n3       0.813725  0.459532  ...  0.596151  0.0  0.326343  0.257362  0.331746   \n4       0.823039  0.333541  ...  0.598763  0.0  0.326786  0.256312  0.337229   \n...          ...       ...  ...       ...  ...       ...       ...       ...   \n478796  0.064622  0.639006  ...  0.683154  0.0  0.300845  0.497901  0.202699   \n478797  0.061671  0.685457  ...  0.680815  0.0  0.300850  0.497868  0.196619   \n478798  0.058250  0.607073  ...  0.677466  0.0  0.301298  0.497555  0.196011   \n478799  0.055304  0.430538  ...  0.683078  0.0  0.300895  0.494969  0.202036   \n478800  0.052926  0.303069  ...  0.688171  0.0  0.301303  0.494597  0.202639   \n\n             C75       C76       C77      C78       C79  \n0       0.916661  0.269393  0.265017  1.00000  0.567254  \n1       0.916661  0.266791  0.251792  1.00000  0.512135  \n2       0.916661  0.265266  0.254707  1.00000  0.469622  \n3       0.916661  0.264379  0.253005  1.00000  0.446285  \n4       0.916661  0.262757  0.247706  1.00000  0.477489  \n...          ...       ...       ...      ...       ...  \n478796  0.111119  0.315343  0.248152  0.26162  0.757619  \n478797  0.111119  0.309817  0.245953  0.26162  0.681373  \n478798  0.111119  0.303620  0.236562  0.26162  0.631425  \n478799  0.111119  0.308706  0.266275  0.26162  0.677024  \n478800  0.111119  0.314736  0.290666  0.26162  0.750658  \n\n[921603 rows x 79 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>C01</th>\n      <th>C02</th>\n      <th>C03</th>\n      <th>C04</th>\n      <th>C05</th>\n      <th>C06</th>\n      <th>C07</th>\n      <th>C08</th>\n      <th>C09</th>\n      <th>C10</th>\n      <th>...</th>\n      <th>C70</th>\n      <th>C71</th>\n      <th>C72</th>\n      <th>C73</th>\n      <th>C74</th>\n      <th>C75</th>\n      <th>C76</th>\n      <th>C77</th>\n      <th>C78</th>\n      <th>C79</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.378953</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.227071</td>\n      <td>0.372380</td>\n      <td>0.000230</td>\n      <td>0.386721</td>\n      <td>0.410567</td>\n      <td>0.784144</td>\n      <td>0.508049</td>\n      <td>...</td>\n      <td>0.584892</td>\n      <td>0.0</td>\n      <td>0.326835</td>\n      <td>0.254687</td>\n      <td>0.331076</td>\n      <td>0.916661</td>\n      <td>0.269393</td>\n      <td>0.265017</td>\n      <td>1.00000</td>\n      <td>0.567254</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.378504</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.226596</td>\n      <td>0.353516</td>\n      <td>0.000161</td>\n      <td>0.399074</td>\n      <td>0.364415</td>\n      <td>0.794139</td>\n      <td>0.540538</td>\n      <td>...</td>\n      <td>0.592044</td>\n      <td>0.0</td>\n      <td>0.326835</td>\n      <td>0.254315</td>\n      <td>0.337223</td>\n      <td>0.916661</td>\n      <td>0.266791</td>\n      <td>0.251792</td>\n      <td>1.00000</td>\n      <td>0.512135</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.378463</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.226789</td>\n      <td>0.318663</td>\n      <td>0.000154</td>\n      <td>0.393283</td>\n      <td>0.451729</td>\n      <td>0.803903</td>\n      <td>0.538802</td>\n      <td>...</td>\n      <td>0.595523</td>\n      <td>0.0</td>\n      <td>0.326387</td>\n      <td>0.255304</td>\n      <td>0.337777</td>\n      <td>0.916661</td>\n      <td>0.265266</td>\n      <td>0.254707</td>\n      <td>1.00000</td>\n      <td>0.469622</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.378904</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.226808</td>\n      <td>0.238782</td>\n      <td>0.000154</td>\n      <td>0.393697</td>\n      <td>0.323289</td>\n      <td>0.813725</td>\n      <td>0.459532</td>\n      <td>...</td>\n      <td>0.596151</td>\n      <td>0.0</td>\n      <td>0.326343</td>\n      <td>0.257362</td>\n      <td>0.331746</td>\n      <td>0.916661</td>\n      <td>0.264379</td>\n      <td>0.253005</td>\n      <td>1.00000</td>\n      <td>0.446285</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.380282</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.226810</td>\n      <td>0.165794</td>\n      <td>0.000154</td>\n      <td>0.412796</td>\n      <td>0.654203</td>\n      <td>0.823039</td>\n      <td>0.333541</td>\n      <td>...</td>\n      <td>0.598763</td>\n      <td>0.0</td>\n      <td>0.326786</td>\n      <td>0.256312</td>\n      <td>0.337229</td>\n      <td>0.916661</td>\n      <td>0.262757</td>\n      <td>0.247706</td>\n      <td>1.00000</td>\n      <td>0.477489</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>478796</th>\n      <td>0.302372</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.703684</td>\n      <td>0.509016</td>\n      <td>0.000538</td>\n      <td>0.420902</td>\n      <td>0.355242</td>\n      <td>0.064622</td>\n      <td>0.639006</td>\n      <td>...</td>\n      <td>0.683154</td>\n      <td>0.0</td>\n      <td>0.300845</td>\n      <td>0.497901</td>\n      <td>0.202699</td>\n      <td>0.111119</td>\n      <td>0.315343</td>\n      <td>0.248152</td>\n      <td>0.26162</td>\n      <td>0.757619</td>\n    </tr>\n    <tr>\n      <th>478797</th>\n      <td>0.304595</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.703684</td>\n      <td>0.485295</td>\n      <td>0.000538</td>\n      <td>0.481569</td>\n      <td>0.335967</td>\n      <td>0.061671</td>\n      <td>0.685457</td>\n      <td>...</td>\n      <td>0.680815</td>\n      <td>0.0</td>\n      <td>0.300850</td>\n      <td>0.497868</td>\n      <td>0.196619</td>\n      <td>0.111119</td>\n      <td>0.309817</td>\n      <td>0.245953</td>\n      <td>0.26162</td>\n      <td>0.681373</td>\n    </tr>\n    <tr>\n      <th>478798</th>\n      <td>0.304373</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.703684</td>\n      <td>0.386965</td>\n      <td>0.000538</td>\n      <td>0.497306</td>\n      <td>0.519990</td>\n      <td>0.058250</td>\n      <td>0.607073</td>\n      <td>...</td>\n      <td>0.677466</td>\n      <td>0.0</td>\n      <td>0.301298</td>\n      <td>0.497555</td>\n      <td>0.196011</td>\n      <td>0.111119</td>\n      <td>0.303620</td>\n      <td>0.236562</td>\n      <td>0.26162</td>\n      <td>0.631425</td>\n    </tr>\n    <tr>\n      <th>478799</th>\n      <td>0.306574</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.703684</td>\n      <td>0.289108</td>\n      <td>0.000538</td>\n      <td>0.468238</td>\n      <td>0.422572</td>\n      <td>0.055304</td>\n      <td>0.430538</td>\n      <td>...</td>\n      <td>0.683078</td>\n      <td>0.0</td>\n      <td>0.300895</td>\n      <td>0.494969</td>\n      <td>0.202036</td>\n      <td>0.111119</td>\n      <td>0.308706</td>\n      <td>0.266275</td>\n      <td>0.26162</td>\n      <td>0.677024</td>\n    </tr>\n    <tr>\n      <th>478800</th>\n      <td>0.305015</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.703684</td>\n      <td>0.248821</td>\n      <td>0.000538</td>\n      <td>0.401886</td>\n      <td>0.366873</td>\n      <td>0.052926</td>\n      <td>0.303069</td>\n      <td>...</td>\n      <td>0.688171</td>\n      <td>0.0</td>\n      <td>0.301303</td>\n      <td>0.494597</td>\n      <td>0.202639</td>\n      <td>0.111119</td>\n      <td>0.314736</td>\n      <td>0.290666</td>\n      <td>0.26162</td>\n      <td>0.750658</td>\n    </tr>\n  </tbody>\n</table>\n<p>921603 rows × 79 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "TRAIN_DF = normalize(TRAIN_DF_RAW[VALID_COLUMNS_IN_TRAIN_DATASET]).ewm(alpha=0.9).mean()\n",
    "TRAIN_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_check(df):\n",
    "    x = np.array(df, dtype=np.float32)\n",
    "    return np.any(x > 1.0), np.any(x < 0), np.any(np.isnan(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(False, False, False)"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "boundary_check(TRAIN_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "datetime.timedelta(seconds=1)"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "dateutil.parser.parse(TRAIN_DF_RAW[TIMESTAMP_FIELD].iloc[1]) - dateutil.parser.parse(TRAIN_DF_RAW[TIMESTAMP_FIELD].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_GIVEN = 59\n",
    "WINDOW_SIZE = 60\n",
    "\n",
    "class HaiDataset(Dataset):\n",
    "    def __init__(self, timestamps, df, stride=1, attacks=None):\n",
    "        self.ts = np.array(timestamps)\n",
    "        self.tag_values = np.array(df, dtype=np.float32)\n",
    "        self.valid_idxs = []\n",
    "\n",
    "        \n",
    "        for L in trange(len(self.ts) - WINDOW_SIZE + 1):\n",
    "            R = L + WINDOW_SIZE - 1\n",
    "            if dateutil.parser.parse(self.ts[R]) - dateutil.parser.parse(\n",
    "                self.ts[L]\n",
    "            ) == timedelta(seconds=WINDOW_SIZE - 1):\n",
    "                self.valid_idxs.append(L)\n",
    "        self.valid_idxs = np.array(self.valid_idxs, dtype=np.int32)[::stride]\n",
    "        self.n_idxs = len(self.valid_idxs)\n",
    "        print(f\"# of valid windows: {self.n_idxs}\")\n",
    "        if attacks is not None:\n",
    "            self.attacks = np.array(attacks, dtype=np.float32)\n",
    "            self.with_attack = True\n",
    "        else:\n",
    "            self.with_attack = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_idxs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.valid_idxs[idx]\n",
    "        last = i + WINDOW_SIZE - 1\n",
    "        item = {\"attack\": self.attacks[last]} if self.with_attack else {}\n",
    "        item[\"ts\"] = self.ts[i + WINDOW_SIZE - 1]\n",
    "        item[\"given\"] = torch.from_numpy(self.tag_values[i : i + WINDOW_GIVEN])\n",
    "        item[\"answer\"] = torch.from_numpy(self.tag_values[last])\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=921544.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "caab4fd136684e85b901d04bfa95f012"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n# of valid windows: 921426\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'ts': '2020-07-11 00:00:59',\n 'given': tensor([[0.3790, 0.0000, 0.0000,  ..., 0.2650, 1.0000, 0.5673],\n         [0.3785, 0.0000, 0.0000,  ..., 0.2518, 1.0000, 0.5121],\n         [0.3785, 0.0000, 0.0000,  ..., 0.2547, 1.0000, 0.4696],\n         ...,\n         [0.3784, 0.0000, 0.0000,  ..., 0.1796, 1.0000, 0.6517],\n         [0.3776, 0.0000, 0.0000,  ..., 0.1777, 1.0000, 0.7075],\n         [0.3761, 0.0000, 0.0000,  ..., 0.1797, 1.0000, 0.6562]]),\n 'answer': tensor([3.7600e-01, 0.0000e+00, 0.0000e+00, 2.2681e-01, 3.3856e-01, 1.5367e-04,\n         4.5900e-01, 4.2068e-01, 9.8894e-01, 5.2564e-01, 5.2291e-01, 1.6963e-01,\n         1.4555e-01, 0.0000e+00, 4.7654e-01, 3.5710e-01, 5.2494e-01, 0.0000e+00,\n         0.0000e+00, 1.7465e-01, 0.0000e+00, 0.0000e+00, 6.2612e-19, 8.8022e-01,\n         0.0000e+00, 4.0426e-01, 3.2757e-01, 0.0000e+00, 2.2855e-01, 1.3690e-01,\n         9.9742e-01, 2.6923e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n         0.0000e+00, 4.8270e-01, 6.0688e-01, 0.0000e+00, 1.0000e+00, 9.1730e-01,\n         0.0000e+00, 5.4634e-01, 3.6803e-01, 5.7016e-01, 6.0934e-01, 0.0000e+00,\n         4.5543e-01, 8.5543e-04, 0.0000e+00, 0.0000e+00, 5.8175e-01, 1.0000e+00,\n         2.1916e-01, 9.9197e-01, 4.9039e-02, 0.0000e+00, 0.0000e+00, 9.9197e-01,\n         0.0000e+00, 5.3105e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4237e-01,\n         0.0000e+00, 9.8961e-01, 8.6530e-01, 5.8835e-01, 0.0000e+00, 2.9396e-01,\n         3.0673e-01, 3.3784e-01, 9.1666e-01, 1.7560e-01, 1.7714e-01, 1.0000e+00,\n         5.0520e-01])}"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "HAI_DATASET_TRAIN = HaiDataset(TRAIN_DF_RAW[TIMESTAMP_FIELD], TRAIN_DF, stride=1)\n",
    "HAI_DATASET_TRAIN[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HIDDENS = 128\n",
    "N_LAYERS = 3\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "class StackedGRU(torch.nn.Module):\n",
    "    def __init__(self, n_tags):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.GRU(\n",
    "            input_size=n_tags,\n",
    "            hidden_size=N_HIDDENS,\n",
    "            num_layers=N_LAYERS,\n",
    "            bidirectional=True,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.lin = torch.nn.Linear(N_HIDDENS * 2, N_HIDDENS * 2)\n",
    "        self.fc = torch.nn.Linear(N_HIDDENS * 2, n_tags)\n",
    "        self.norm = torch.nn.BatchNorm1d(N_HIDDENS * 2)\n",
    "        self.elu = torch.nn.ELU()\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(0, 1)  # (batch, seq, params) -> (seq, batch, params)\n",
    "        self.rnn.flatten_parameters()\n",
    "        outs, _ = self.rnn(x)\n",
    "        nom = self.norm(outs[-1])\n",
    "        out1 = self.elu(self.lin(nom))\n",
    "        \n",
    "        out = self.fc(out1)\n",
    "        return x[0] + out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "StackedGRU(\n  (rnn): GRU(79, 128, num_layers=3, dropout=0.2, bidirectional=True)\n  (lin): Linear(in_features=256, out_features=256, bias=True)\n  (fc): Linear(in_features=256, out_features=79, bias=True)\n  (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (elu): ELU(alpha=1.0)\n)"
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "MODEL = StackedGRU(n_tags=TRAIN_DF.shape[1])\n",
    "MODEL.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, model, batch_size, n_epochs):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=5e-5,weight_decay=0.2)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    epochs = trange(n_epochs, desc=\"training\")\n",
    "    best = {\"loss\": sys.float_info.max}\n",
    "    loss_history = []\n",
    "    for e in epochs:\n",
    "        epoch_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            given = batch[\"given\"].cuda()\n",
    "            guess = model(given)\n",
    "            answer = batch[\"answer\"].cuda()\n",
    "            loss = loss_fn(answer, guess)\n",
    "            loss.backward()\n",
    "            epoch_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        loss_history.append(epoch_loss)\n",
    "        epochs.set_postfix_str(f\"loss: {epoch_loss:.6f}\")\n",
    "        if epoch_loss < best[\"loss\"]:\n",
    "            best[\"state\"] = model.state_dict()\n",
    "            best[\"loss\"] = epoch_loss\n",
    "            best[\"epoch\"] = e + 1\n",
    "    return best, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='training', max=32.0, style=ProgressStyle(description_widt…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e8e6e1c2c1e44059c0641666a888943"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-94-c85f7a73ffde>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataset, model, batch_size, n_epochs)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mgiven\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"given\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mguess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgiven\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"answer\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MODEL.train()\n",
    "BEST_MODEL, LOSS_HISTORY = train(HAI_DATASET_TRAIN, MODEL, BATCH_SIZE, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL[\"loss\"], BEST_MODEL[\"epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.pt\", \"wb\") as f:\n",
    "    torch.save(\n",
    "        {\n",
    "            \"state\": BEST_MODEL[\"state\"],\n",
    "            \"best_epoch\": BEST_MODEL[\"epoch\"],\n",
    "            \"loss_history\": LOSS_HISTORY,\n",
    "        },\n",
    "        f,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.pt\", \"rb\") as f:\n",
    "    SAVED_MODEL = torch.load(f)\n",
    "\n",
    "MODEL.load_state_dict(SAVED_MODEL[\"state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "plt.title(\"Training Loss Graph\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(SAVED_MODEL[\"loss_history\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_DF_RAW = dataframe_from_csvs(VALIDATION_DATASET)\n",
    "VALIDATION_DF_RAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VALIDATION_DF = normalize(VALIDATION_DF_RAW[VALID_COLUMNS_IN_TRAIN_DATASET])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_check(VALIDATION_DF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HAI_DATASET_VALIDATION = HaiDataset(\n",
    "    VALIDATION_DF_RAW[TIMESTAMP_FIELD], VALIDATION_DF, attacks=VALIDATION_DF_RAW[ATTACK_FIELD]\n",
    ")\n",
    "HAI_DATASET_VALIDATION[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset, model, batch_size):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    ts, dist, att = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            given = batch[\"given\"].cuda()\n",
    "            answer = batch[\"answer\"].cuda()\n",
    "            guess = model(given)\n",
    "            ts.append(np.array(batch[\"ts\"]))\n",
    "            dist.append(torch.abs(answer - guess).cpu().numpy())\n",
    "            try:\n",
    "                att.append(np.array(batch[\"attack\"]))\n",
    "            except:\n",
    "                att.append(np.zeros(batch_size))\n",
    "            \n",
    "    return (\n",
    "        np.concatenate(ts),\n",
    "        np.concatenate(dist),\n",
    "        np.concatenate(att),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "MODEL.eval()\n",
    "CHECK_TS, CHECK_DIST, CHECK_ATT = inference(HAI_DATASET_VALIDATION, MODEL, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECK_DIST.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANOMALY_SCORE = np.mean(CHECK_DIST, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_graph(xs, att, piece=2, THRESHOLD=None):\n",
    "    l = xs.shape[0]\n",
    "    chunk = l // piece\n",
    "    fig, axs = plt.subplots(piece, figsize=(20, 4 * piece))\n",
    "    for i in range(piece):\n",
    "        L = i * chunk\n",
    "        R = min(L + chunk, l)\n",
    "        xticks = range(L, R)\n",
    "        axs[i].plot(xticks, xs[L:R])\n",
    "        if len(xs[L:R]) > 0:\n",
    "            peak = max(xs[L:R])\n",
    "            axs[i].plot(xticks, att[L:R] * peak * 0.3)\n",
    "        if THRESHOLD!=None:\n",
    "            axs[i].axhline(y=THRESHOLD, color='r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.04\n",
    "check_graph(ANOMALY_SCORE, CHECK_ATT, piece=2, THRESHOLD=THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_labels(distance, threshold):\n",
    "    xs = np.zeros_like(distance)\n",
    "    xs[distance > threshold] = 1\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = put_labels(ANOMALY_SCORE, THRESHOLD)\n",
    "LABELS, LABELS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTACK_LABELS = put_labels(np.array(VALIDATION_DF_RAW[ATTACK_FIELD]), threshold=0.5)\n",
    "ATTACK_LABELS, ATTACK_LABELS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fill_blank(check_ts, labels, total_ts):\n",
    "    def ts_generator():\n",
    "        for t in total_ts:\n",
    "            yield dateutil.parser.parse(t)\n",
    "\n",
    "    def label_generator():\n",
    "        for t, label in zip(check_ts, labels):\n",
    "            yield dateutil.parser.parse(t), label\n",
    "\n",
    "    g_ts = ts_generator()\n",
    "    g_label = label_generator()\n",
    "    final_labels = []\n",
    "\n",
    "    try:\n",
    "        current = next(g_ts)\n",
    "        ts_label, label = next(g_label)\n",
    "        while True:\n",
    "            if current > ts_label:\n",
    "                ts_label, label = next(g_label)\n",
    "                continue\n",
    "            elif current < ts_label:\n",
    "                final_labels.append(0)\n",
    "                current = next(g_ts)\n",
    "                continue\n",
    "            final_labels.append(label)\n",
    "            current = next(g_ts)\n",
    "            ts_label, label = next(g_label)\n",
    "    except StopIteration:\n",
    "        return np.array(final_labels, dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "FINAL_LABELS = fill_blank(CHECK_TS, LABELS, np.array(VALIDATION_DF_RAW[TIMESTAMP_FIELD]))\n",
    "FINAL_LABELS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTACK_LABELS.shape[0] == FINAL_LABELS.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TaPR = etapr.evaluate(anomalies=ATTACK_LABELS, predictions=FINAL_LABELS)\n",
    "print(f\"F1: {TaPR['f1']:.3f} (TaP: {TaPR['TaP']:.3f}, TaR: {TaPR['TaR']:.3f})\")\n",
    "print(f\"# of detected anomalies: {len(TaPR['Detected_Anomalies'])}\")\n",
    "print(f\"Detected anomalies: {TaPR['Detected_Anomalies']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DF_RAW = dataframe_from_csvs(TEST_DATASET)\n",
    "TEST_DF_RAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEST_DF = normalize(TEST_DF_RAW[VALID_COLUMNS_IN_TRAIN_DATASET]).ewm(alpha=0.9).mean()\n",
    "TEST_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_check(TEST_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HAI_DATASET_TEST = HaiDataset(\n",
    "    TEST_DF_RAW[TIMESTAMP_FIELD], TEST_DF, attacks=None\n",
    ")\n",
    "HAI_DATASET_VALIDATION[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "MODEL.eval()\n",
    "CHECK_TS, CHECK_DIST, CHECK_ATT = inference(HAI_DATASET_TEST, MODEL, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANOMALY_SCORE = np.mean(CHECK_DIST, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_graph(ANOMALY_SCORE, CHECK_ATT, piece=3, THRESHOLD=THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = put_labels(ANOMALY_SCORE, THRESHOLD)\n",
    "LABELS, LABELS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission.index = submission['time']\n",
    "submission.loc[CHECK_TS,'attack'] = LABELS\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python37464bittorchcondafde7a5aef0a6491ea8e9b01db5497bc6",
   "display_name": "Python 3.7.4 64-bit ('torch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "09962ad74437132adee2b5ad95b01902fb99f850ad45a831e9d347639ab493fc"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}