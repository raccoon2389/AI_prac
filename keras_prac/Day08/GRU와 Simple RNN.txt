LSTM과 GRU의 기초 모델이 되는 simpleRNN이 있다

simpleRNN이란 input데이터가 다음의 input data가 weight값과 연산한후 결과가 되고 그 결과값이 새로운 h가 된다.
h에서 weight값을 연산하여 결과값을 뽑아내고 h를 다음 유닛으로 넘긴다
recursive한 구조로 input data가 또 흘러들오온 h와 연산하게 된다.

LSTM 이란

거대한 컨데어 벨트같은 cell state가 존재한다.
이 cell state는 LSTM을 하려금 다음 유닛들로 흘러가면서 전 유닛의 데이터를 활용할수도 있고 안할수도 있는 특징을 갖을수 있게 만들어준다
1. cell state는 반복모듈(유닛)을 단방향으로 이동하며 각 유닛에 들어가서 input과 h(t-1)[전 결과]를 concentate하고 Wf로 연산후 시그모이드를 취해 전 유닛으로부터 흘러 들어온 cell state와 곱하고 그것을 그대로 흘려주어서 전 cell state를 얼마나 반영할 것 인지를 정한다
2. 어느 방향으로 업데이트 할지 정하는 C 와 얼마나 가야할지 정하는 i와 연산후 cell state에 반영한다 (이 시점에서 새로운 cell state로 update된다. 즉 C(t-1)->C(t))
3. concentated한 데이터를 o와 연산하고 시그모이드 한다음 cell state과 곱한값여 cell state에서 어떤 데이터를 뽑아서 output에 내보낼지(h) 결정한다.

전 유닛으로 부터 흘러들어온 cell state를 보존하고 이용 할지말지 정하는 f, cell state의 update에 대한 방향성(C)와 세기(i), output에 어떤 data를 내보낼지 결정하는 o 이렇게 반복모듈 안에 4개의 layer 와 그안의 weight 또 각각 weight의 bias가 존재한다

그러므로 총 파라미터의 갯수는 (입력 차원 + 반복 모듈갯수 + bias(1) )*(반복 모듈갯수)*(layer안의 weight 갯수=4)이 된다.

출처 : http://colah.github.io/posts/2015-08-Understanding-LSTMs/


GRU

LSTM과 다르게 결과값 h 와 Cell state c 가 하나의 벡터 h로 합쳐졌다
forget 과 input을 한번에 조정한다.
파라미터의 총 개수는 (input + bias + output)*output이 된다

https://arxiv.org/pdf/1406.1078v3.pdf
